#!/usr/bin/python3

import argparse
import json
import sys
import os
import re
import concurrent.futures
import pathlib
import requests
import bs4


class bcolors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


green = bcolors.OKGREEN if sys.stdout.isatty() else ""
blue = bcolors.OKBLUE if sys.stdout.isatty() else ""
cyan = bcolors.OKCYAN if sys.stdout.isatty() else ""


class Argparser(object):
    def __init__(self):
        parser = argparse.ArgumentParser()
        parser.add_argument("--url", type=str, nargs="+", help="url to scrape")
        parser.add_argument(
            "--name", type=str, nargs="+", help="url to scrape"
        )
        parser.add_argument(
            "--dbg", action="store_true", help="debug", default=False
        )
        parser.add_argument(
            "--demon", action="store_true", help="run as daemon", default=False
        )
        parser.add_argument(
            "--manga",
            action="store_true",
            help="run manga scraper",
            default=False,
        )
        parser.add_argument(
            "--anime",
            action="store_true",
            help="run anime scraper",
            default=False,
        )
        parser.add_argument(
            "--cb", action="store_true", help="run cb scraper", default=False
        )
        parser.add_argument(
            "--slumber", type=int, help="how long the demon sleeps", default=60
        )
        self.args = parser.parse_args()


path = str()
if pathlib.Path(sys.argv[0]).is_symlink():
    path = os.readlink(sys.argv[0])
else:
    path = sys.argv[0]


def single_get(url: str) -> requests.Response:
    return requests.get(url, allow_redirects=True)


def multi_get(urls: list) -> list:
    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as pool:
        response_list = list(pool.map(single_get, urls))
    return response_list


def single_get_tag(input: list) -> (requests.Response, str):
    return requests.get(input[0], allow_redirects=True), input[1]


def multi_get_tag(urls: list) -> list:
    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as pool:
        response_list = list(pool.map(single_get_tag, urls))
    return response_list


def mrg(url: str) -> bool:
    requests.packages.urllib3.disable_warnings()
    resp = requests.get(url, verify=False)
    soup = bs4.BeautifulSoup(resp.text, "lxml")
    search = soup.find_all("div", class_="label")

    Up_Time = str()
    for elem in search:
        if elem.string == "Last Broadcast:":
            Up_Time = elem.next_sibling.next_sibling.string
            # print(elem.next_sibling.next_sibling)

    if Up_Time.find("minutes") != -1:
        return True
    else:
        return False


def run_cb_scrape() -> None:
    url = json.load(open(os.path_dirname(path) + "/cb.json"))
    if mrg(url["1"]):
        print("mg ", end="")
        vocalize(os.path.expanduser("~") + "/scripts/mila/mgup.ogg")
    if mrg(url["2"]):
        print("obk ", end="")
        vocalize(os.path.expanduser("~") + "/scripts/mila/obkup.ogg")
    if mrg(url["5"]):
        print("ls ", end="")
        vocalize(os.path.expanduser("~") + "/scripts/mila/lisaup.ogg")


def manga_scrape() -> None:
    urls = json.load(open(os.path.dirname(path) + "/manga.json"))
    requests.packages.urllib3.disable_warnings()
    result = str()
    url_list = list()
    [url_list.append(url) for _, url in urls.items()]
    response_list = multi_get(url_list)

    for resp in response_list:
        soup = bs4.BeautifulSoup(resp.text, "lxml")
        search = soup.find_all("a", class_="chapter-name text-nowrap")
        re_res = []
        for thing in search:
            re_res.append(
                green + thing["title"] + " > " + blue + thing["href"]
            )
        try:
            result += re_res[0] + "\n"
        except IndexError:
            pass
            # result += thing["title"] + "--> nothing\n"
    print(result, end="")


def anime_scrape() -> None:
    urls = json.load(open(os.path.dirname(path) + "/anime.json"))
    requests.packages.urllib3.disable_warnings()
    results = list()
    url_list = list()
    [url_list.append([url, tag]) for tag, url in urls.items()]
    response_list = multi_get_tag(url_list)

    for resp, tag in response_list:
        soup = bs4.BeautifulSoup(resp.text, "lxml")
        search = soup.find_all("a")
        re_res = []

        for thing in search:
            child = thing.findChild("span", class_="jtitle")
            if not child:
                continue
            re_res.append(re.findall("Episode [0-9]*$", thing.text))
        results.append(green + tag + " > " + blue + repr(max(re_res)))
    for result in results:
        print(result)


def vocalize(sound) -> None:
    # import subprocess
    # subprocess.call([os.path.expanduser("~") + "/scripts/voice.sh", sound])
    pass


def main():
    argparser = Argparser()
    if argparser.args.cb:
        run_cb_scrape()
    elif argparser.args.manga:
        manga_scrape()
    elif argparser.args.anime:
        anime_scrape()
    else:
        pass


if __name__ == "__main__":
    main()
